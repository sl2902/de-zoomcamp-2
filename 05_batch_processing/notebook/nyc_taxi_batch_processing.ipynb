{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784572b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import SparkFiles\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42500d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caee5d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/02 02:01:39 WARN Utils: Your hostname, sls-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.29.248 instead (on interface en0)\n",
      "24/03/02 02:01:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/02 02:01:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "            .master(\"local[*]\")\n",
    "            .appName(\"NYC Taxi Batch processing\")\n",
    "            .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ce2b90",
   "metadata": {},
   "source": [
    "Q1\n",
    "\n",
    "What is the version of Spark in use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a6d5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668b2b7",
   "metadata": {},
   "source": [
    "Q2\n",
    "\n",
    "FHV October 2019\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28d57f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-10.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2062a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addFile(file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4395a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "            [\n",
    "                StructField(\"dispatching_base_num\", StringType()),\n",
    "                StructField(\"pickup_datetime\", TimestampType()),\n",
    "                StructField(\"dropoff_datetime\", TimestampType()),\n",
    "                StructField(\"PULocationID\", IntegerType()),\n",
    "                StructField(\"DOLocationID\", IntegerType()),\n",
    "                StructField(\"SR_Flag\", StringType()),\n",
    "                StructField(\"Affiliated_base_number\", StringType())\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb46b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to counter the following error\n",
    "# Parquet column cannot be converted in file \n",
    "# file:///private/var/folders/tb/tknsg8m10jvc1t4fztzvf7kh0000gn/T/spark-ee38c745-46da-4d4a-9d36-0b8860405f64/userFiles-40720fdf-cdc1-40e1-859d-1da9e4f4cf59/fhv_tripdata_2019-10.parquet. Column: [PUlocationID], Expected: int, Found: DOUBLE\n",
    "# below config doesn't fix the error\n",
    "# spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f0c96d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    spark\n",
    "        .read\n",
    "        # .schema(schema)\n",
    "        .parquet(SparkFiles.get(file_url.split(\"/\")[-1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2d960e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- dropOff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- PUlocationID: double (nullable = true)\n",
      " |-- DOlocationID: double (nullable = true)\n",
      " |-- SR_Flag: integer (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada86066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1897856"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f68cc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2484c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df.write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(\"../data/fhv/2019/10\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe8710a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 74496\n",
      "-rw-r--r--  1 home  staff     0B Mar  2 02:01 _SUCCESS\n",
      "-rw-r--r--  1 home  staff   5.7M Mar  2 02:01 part-00000-bfff2089-876d-4ae8-8585-5b21bbbd01c5-c000.snappy.parquet\n",
      "-rw-r--r--  1 home  staff   5.6M Mar  2 02:01 part-00001-bfff2089-876d-4ae8-8585-5b21bbbd01c5-c000.snappy.parquet\n",
      "-rw-r--r--  1 home  staff   5.7M Mar  2 02:01 part-00002-bfff2089-876d-4ae8-8585-5b21bbbd01c5-c000.snappy.parquet\n",
      "-rw-r--r--  1 home  staff   5.7M Mar  2 02:01 part-00003-bfff2089-876d-4ae8-8585-5b21bbbd01c5-c000.snappy.parquet\n",
      "-rw-r--r--  1 home  staff   5.7M Mar  2 02:01 part-00004-bfff2089-876d-4ae8-8585-5b21bbbd01c5-c000.snappy.parquet\n",
      "-rw-r--r--  1 home  staff   5.7M Mar  2 02:01 part-00005-bfff2089-876d-4ae8-8585-5b21bbbd01c5-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -lh \"../data/fhv/2019/10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f0a1aa",
   "metadata": {},
   "source": [
    "Q3\n",
    "\n",
    "Count records\n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c656a4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62629"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .filter(col(\"pickup_datetime\").cast(DateType()) == '2019-10-15')\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb10046",
   "metadata": {},
   "source": [
    "Q4\n",
    "\n",
    "Longest trip for each day\n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68013bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n",
      "|pickup_date|max_duration_hrs|\n",
      "+-----------+----------------+\n",
      "| 2019-10-28|        631152.5|\n",
      "+-----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .select(col(\"dropoff_datetime\"), col(\"pickup_datetime\"))\n",
    "    .withColumn(\"duration\", (col(\"dropoff_datetime\") - col(\"pickup_datetime\")).cast(LongType())/3600)\n",
    "    .withColumn(\"pickup_date\", to_date(col(\"pickup_datetime\")))\n",
    "    .groupBy(\"pickup_date\")\n",
    "    .max(\"duration\")\n",
    "    .withColumnRenamed(\"max(duration)\", \"max_duration_hrs\")\n",
    "    .orderBy('max_duration_hrs', ascending=False) \\\n",
    "    .limit(1)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d8c46",
   "metadata": {},
   "source": [
    "Q5\n",
    "\n",
    "http://localhost:4040"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3122ca5",
   "metadata": {},
   "source": [
    "Q6:\n",
    "Least frequent pickup location zone\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark\n",
    "Zone Data\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7a0dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.csv('../../input_data/taxi_zone_lookup.csv', header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1de8bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('fhv_2019_10')\n",
    "df_zones.createOrReplaceTempView('taxi_trip_zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3700fb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                zone|num_pickups|\n",
      "+--------------------+-----------+\n",
      "|         Jamaica Bay|          1|\n",
      "|Governor's Island...|          2|\n",
      "| Green-Wood Cemetery|          5|\n",
      "+--------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select\n",
    "     zone,\n",
    "     count(*) num_pickups\n",
    "    from\n",
    "     fhv_2019_10 a left join taxi_trip_zones b on a.PULocationID = cast(b.LocationID as double)\n",
    "    group by\n",
    "     zone\n",
    "    order by\n",
    "     num_pickups asc\n",
    "    \"\"\"\n",
    ").show(3)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94aaa73-d972-4266-a884-8876602a0178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "05_batch_processing-ek8kXzFk",
   "language": "python",
   "name": "05_batch_processing-ek8kxzfk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
